import os
import random
from pathlib import Path

import torch
import numpy as np
from torch.optim import Adam, SGD
from torch.utils.data import DataLoader
from tqdm import tqdm
import random
from dataset.name2dataset import name2dataset
from network.loss import name2loss
from network.renderer import name2renderer
from train.lr_common_manager import name2lr_manager
from network.metrics import name2metrics
from train.train_tools import to_cuda, Logger
from train.train_valid import ValidationEvaluator
from utils.dataset_utils import dummy_collate_fn

class Meshlabserver:
    def __init__(self):
        # script generated by meshlab-2020.04
        self.meshlab_remesh_srcipt = """
        <!DOCTYPE FilterScript>
        <FilterScript>
         <filter name="Remeshing: Isotropic Explicit Remeshing">
          <Param value="3" isxmlparam="0" name="Iterations" type="RichInt" description="Iterations" tooltip="Number of iterations of the remeshing operations to repeat on the mesh."/>
          <Param value="false" isxmlparam="0" name="Adaptive" type="RichBool" description="Adaptive remeshing" tooltip="Toggles adaptive isotropic remeshing."/>
          <Param value="false" isxmlparam="0" name="SelectedOnly" type="RichBool" description="Remesh only selected faces" tooltip="If checked the remeshing operations will be applied only to the selected faces."/>
          <Param value="{}" isxmlparam="0" name="TargetLen" type="RichAbsPerc" description="Target Length" min="0" max="1.6" tooltip="Sets the target length for the remeshed mesh edges."/>
          <Param value="180" isxmlparam="0" name="FeatureDeg" type="RichFloat" description="Crease Angle" tooltip="Minimum angle between faces of the original to consider the shared edge as a feature to be preserved."/>
          <Param value="true" isxmlparam="0" name="CheckSurfDist" type="RichBool" description="Check Surface Distance" tooltip="If toggled each local operation must deviate from original mesh by [Max. surface distance]"/>
          <Param value="0.002" isxmlparam="0" name="MaxSurfDist" type="RichAbsPerc" description="Max. Surface Distance" min="0" max="1.6" tooltip="Maximal surface deviation allowed for each local operation"/>
          <Param value="true" isxmlparam="0" name="SplitFlag" type="RichBool" description="Refine Step" tooltip="If checked the remeshing operations will include a refine step."/>
          <Param value="true" isxmlparam="0" name="CollapseFlag" type="RichBool" description="Collapse Step" tooltip="If checked the remeshing operations will include a collapse step."/>
          <Param value="true" isxmlparam="0" name="SwapFlag" type="RichBool" description="Edge-Swap Step" tooltip="If checked the remeshing operations will include a edge-swap step, aimed at improving the vertex valence of the resulting mesh."/>
          <Param value="true" isxmlparam="0" name="SmoothFlag" type="RichBool" description="Smooth Step" tooltip="If checked the remeshing operations will include a smoothing step, aimed at relaxing the vertex positions in a Laplacian sense."/>
          <Param value="true" isxmlparam="0" name="ReprojectFlag" type="RichBool" description="Reproject Step" tooltip="If checked the remeshing operations will include a step to reproject the mesh vertices on the original surface."/>
         </filter>
        </FilterScript>
        """
        pid = str(os.getpid())
        path = '/home/sunjiamu/NeRO/tmp'
        self.ply_path = f"{path}/temp_{pid}.ply"
        self.remeshply_path = f"{path}/remesh_{pid}.ply"
        self.script_path = f"{path}/script_{pid}.mlx"
        self.cmd = '/home/sunjiamu/DRT/MeshLabServer2020.07-linux.AppImage' + \
            ' -i ' + self.ply_path + \
            ' -o ' + self.remeshply_path + \
            ' -s ' + self.script_path
        self.cmd = self.cmd + " 1>/dev/null 2>&1"
        print(self.cmd)
        
    def remesh(self, scene, remesh_len):
        meshlab_remesh_srcipt = self.meshlab_remesh_srcipt.format(remesh_len)
        with open(self.script_path, 'w') as script_file:
            script_file.write(meshlab_remesh_srcipt)
        scene.mesh.export(self.ply_path)
        assert(os.system(self.cmd) == 0)
        print('updated mesh' + self.remeshply_path)
        scene.update_mesh(self.remeshply_path) 

class Trainer:
    default_cfg = {
        "optimizer_type": 'adam',
        "multi_gpus": False,
        "lr_type": "exp_decay",
        "lr_cfg": {
            "lr_init": 2.0e-4,
            "lr_step": 100000,
            "lr_rate": 0.5,
        },
        "total_step": 300000,
        "train_log_step": 20,
        "val_interval": 10000,
        "save_interval": 500,
        "novel_view_interval": 10000,
        "worker_num": 8,
        'random_seed': 16033,
    }

    def _init_dataset(self):
        self.train_set = name2dataset[self.cfg['train_dataset_type']](self.cfg['train_dataset_cfg'], True)
        self.train_set = DataLoader(self.train_set, 1, True, num_workers=self.cfg['worker_num'],
                                    collate_fn=dummy_collate_fn)
        print(f'train set len {len(self.train_set)}')
        self.val_set_list, self.val_set_names = [], []
        dataset_dir = self.cfg['dataset_dir']
        for val_set_cfg in self.cfg['val_set_list']:
            name, val_type, val_cfg = val_set_cfg['name'], val_set_cfg['type'], val_set_cfg['cfg']
            val_set = name2dataset[val_type](val_cfg, False, dataset_dir=dataset_dir)
            val_set = DataLoader(val_set, 1, False, num_workers=self.cfg['worker_num'], collate_fn=dummy_collate_fn)
            self.val_set_list.append(val_set)
            self.val_set_names.append(name)
            print(f'{name} val set len {len(val_set)}')

    def _init_network(self):
       # print('x')
        self.network = name2renderer[self.cfg['network']](self.cfg).cuda()

        # loss
        self.val_losses = []
        for loss_name in self.cfg['loss']:
            self.val_losses.append(name2loss[loss_name](self.cfg))
        self.val_metrics = []
       # print('y')
        # metrics
        for metric_name in self.cfg['val_metric']:
            if metric_name in name2metrics:
                self.val_metrics.append(name2metrics[metric_name](self.cfg))
            else:
                self.val_metrics.append(name2loss[metric_name](self.cfg))

        # we do not support multi gpu training for NeuRay
        if self.cfg['multi_gpus']:
            raise NotImplementedError
            # make multi gpu network
            # self.train_network=DataParallel(MultiGPUWrapper(self.network,self.val_losses))
            # self.train_losses=[DummyLoss(self.val_losses)]
        else:
            self.train_network = self.network
            self.train_losses = self.val_losses

        if self.cfg['optimizer_type'] == 'adam':
            self.optimizer = Adam
        elif self.cfg['optimizer_type'] == 'sgd':
            self.optimizer = SGD
        else:
            raise NotImplementedError

        self.val_evaluator = ValidationEvaluator(self.cfg)
        self.lr_manager = name2lr_manager[self.cfg['lr_type']](self.cfg['lr_cfg'])
        self.optimizer = self.lr_manager.construct_optimizer(self.optimizer, self.network)

    def __init__(self, cfg):
        self.cfg = {**self.default_cfg, **cfg}
        torch.manual_seed(self.cfg['random_seed'])
        np.random.seed(self.cfg['random_seed'])
        random.seed(self.cfg['random_seed'])
        self.model_name = cfg['name']
        self.model_dir = os.path.join('data/model', cfg['name'])
        if not os.path.exists(self.model_dir): Path(self.model_dir).mkdir(exist_ok=True, parents=True)
        self.pth_fn = os.path.join(self.model_dir, 'model.pth')
        self.best_pth_fn = os.path.join(self.model_dir, 'model_best.pth')
        self.meshlabserver = Meshlabserver()

    def run(self):
       # print(1)
        self._init_dataset()
        self._init_network()
        self._init_logger()
      #  print(2)
        best_para, start_step = self._load_model()
        train_iter = iter(self.train_set)

        pbar = tqdm(total=self.cfg['total_step'], bar_format='{r_bar}')
        pbar.update(start_step)
        #for name, param in self.network.named_parameters():
        #    param.requires_grad = True if name.find('sdf_network') == -1 else False 
        # for name, param in self.network.named_parameters():
        #     if name.find('infinity_far_bkgr') != -1:
        #         print(name)
        #     param.requires_grad = True if name.find('infinity_far_bkgr') == -1 else False 

       # self.network.color_network.bkgr = self.network.stage1_network.color_network.bkgr
        for name, param in self.network.stage1_network.named_parameters():
           param.requires_grad = False
        self.network.IORs.requires_grad=False
        IOR_history = []

       # print(3)
        def interp_L(start, end, it, Pass):
            assert it <= Pass-1
            step = (end - start)/(Pass-1)
            return it*step + start

        def interp_R(start, end, it, Pass):
            return 1/interp_L(1/start, 1/end, it, Pass)

        def limit_hook(grad):
            max = 0.5
            if torch.isnan(grad).any():
                print("nan in grad")
            grad[torch.isnan(grad)] = 0
            grad[grad>max]=max
            grad[grad<-max]=-max
            return grad


        def setup_opt( scene, lr):

            init_vertices = scene.vertices
           # rand_init = (0.005 * np.random.rand(init_vertices.shape[0]).reshape(-1,1).repeat(3,axis=-1)) * scene.normals.detach().cpu().numpy()

           # rand_init = 0.01 * np.random.rand(init_vertices.shape[0]).reshape(-1,1).repeat(3,axis=-1)
          #  parameter = torch.tensor(rand_init,requires_grad=True,device='cuda:0')
          #  zero_init = torch.zeros((init_vertices.shape[0],3),device='cuda:0')#.expand(init_vertices.shape[0],3)
            zero_init = torch.zeros((init_vertices.shape[0],1),device='cuda:0')#.expand(init_vertices.shape[0],3)
            parameter = torch.tensor(zero_init,requires_grad=True,device='cuda:0')
          #  parameter.register_hook(limit_hook)
          #  opt = torch.optim.Adam([parameter], lr=lr)
            opt = torch.optim.SGD([parameter], lr=lr,momentum=0.95,nesterov=True)
            return init_vertices, parameter, opt, scene.normals.detach()
        


        for step in range(start_step, self.cfg['total_step']):
            #print(4)

            if (step) % 50000 == 0:
                remesh_len = interp_R(0.01, 0.003, step, 10 * 50000)
              #  remesh_len = 0.03
                lr = interp_R(1e-2, 0.1 * 1e-2,  step, 10 * 50000)
                modelname = self.cfg['name']
                os.makedirs(f'data/train_vis/{modelname}-val/',exist_ok=True)
                self.network.scene.mesh.export(f'data/train_vis/{modelname}-val/{step}_orig.ply')
                print(f'remesh_len {remesh_len:g} lr {lr:g}')
                self.meshlabserver.remesh(self.network.scene, remesh_len)
                init_vertices, parameter, opt, normals_init = setup_opt(self.network.scene, lr)
                
                
                self.network.scene.mesh.export(f'data/train_vis/{modelname}-val/{step}.ply')
           # if step >= 60000: exit(1)
           # if step >= 50000:
           #     for name, param in self.network.named_parameters():
           #         param.requires_grad = True
       

            try:
                train_data = next(train_iter)
            except StopIteration:
                self.train_set.dataset.reset()
                train_iter = iter(self.train_set)
                train_data = next(train_iter)
            if not self.cfg['multi_gpus']:
                train_data = to_cuda(train_data)
            train_data['step'] = step

            self.train_network.train()
            self.network.train()
            lr = self.lr_manager(self.optimizer, step)

            self.optimizer.zero_grad()
            self.train_network.zero_grad()
            opt.zero_grad()

         #   print(5)
            vertices = init_vertices + ((torch.sigmoid(parameter) - 0.5) * 0.1).reshape(init_vertices.shape[0],1) * normals_init
            self.network.scene.update_verticex(vertices)
            if step == 0:
                self.network.scene.mesh.export(f'data/train_vis/{modelname}-val/0_rand.ply')
            if torch.isnan(vertices).any():
                print("nan in vertices")


            
           # print(6)
            # if (step + 1) % self.cfg['novel_view_interval'] == 0:
            #     render_data = train_data.copy()
            #     render_data["render"] = True

            #     self.train_network(render_data)
           # with torch.autograd.detect_anomaly():
            log_info = {}
            outputs = self.train_network(train_data)
            for loss in self.train_losses:
                loss_results = loss(outputs, train_data, step)
                for k, v in loss_results.items():
                    log_info[k] = v
        
            loss = 0
       #     dihedral_angle = torch.clamp(self.network.scene.dihedral_angle(),0.,1.) # cosine of angle [-1,1]
            dihedral_angle = self.network.scene.dihedral_angle()
            #dihedral_angle = 1-dihedral_angle
            dihedral_angle = -torch.log(1+dihedral_angle)
            sm_loss = dihedral_angle.sum()
            vh_loss = 0
          #  print(7)
            for V_index in random.sample(list(range(250)),20):
                pose = torch.tensor(self.network.database.get_pose_orig(V_index),device='cuda:0').float()
                mask = torch.tensor(self.network.database.get_mask(V_index),device='cuda:0').float()
                K = torch.tensor(self.network.database.get_K(0),device='cuda:0').float()
                camera_M = (pose, K)
                #print(camera_M)
               
                #exit(1)
                
                
                origin = pose[:3,3:4]
                silhouette_edge = self.network.scene.silhouette_edge(origin.flatten())
                index, output = self.network.scene.primary_visibility(silhouette_edge,camera_M ,origin.flatten(), detach_depth=True)
                #print(index,output)
                vh_loss += (mask.view((2048,2048))[index[:,1],index[:,0]] - output).abs().sum()
                # xx = -np.ones((512,512,3))
                # for ii in range(512):
                #     xx[ii,:,0] = ii
                # for jj in range(512):
                #     xx[:,jj, 1] = jj

                # color_grid = np.zeros((512,512,3))
                # for k in range(512):
                #     for m in range(512):
                #         color_grid[k,m,0] = mask[m,k] * 255
                #         color_grid[k,m,1] = mask[m,k] * 255
                #         color_grid[k,m,2] = mask[m,k] * 255

                # color = mask.view((512,512))[index[:,0],index[:,1]].reshape(-1,1).expand(index.shape[0],3)

                # import trimesh
                # index_3d = torch.cat([index,torch.zeros((index.shape[0],1), device='cuda:0')],-1)
                # pc = trimesh.PointCloud(vertices = index_3d.detach().cpu().numpy(),colors=color.detach().cpu().numpy() * 255)
                # trimesh.exchange.export.export_mesh(pc,'pc_index_color.ply')

            
                # pc2 = trimesh.PointCloud(vertices = xx.reshape(-1,3),colors=color_grid.reshape(-1,3))
                # trimesh.exchange.export.export_mesh(pc2,'pc_mask.ply')
                # exit(1)
            loss += 0.0005 * vh_loss / 1024
                

            loss += 0.025 * self.network.scene.mean_len/10 * sm_loss
            for k, v in log_info.items():
                #print(k)
                #print(v)
                if k.startswith('loss'):
                    loss = loss + torch.mean(v)

            loss.backward()
            opt.step()
            # for name, param in self.network.named_parameters():
            #     if name.find('infinity_far_bkgr') != -1:
            #         param.grad[:] = 0
     #       for name, param in self.network.named_parameters():
         #       if name.find('sdf_network') != -1:
         #           param.grad[:] = 0
            

            #exit(1)
            self.optimizer.step()
            if ((step + 1) % self.cfg['train_log_step']) == 0:
                self._log_data(log_info, step + 1, 'train')

            if step == 0 or (step + 1) % self.cfg['val_interval'] == 0 or (step + 1) == self.cfg['total_step']:
               # print(self.network.IORs)
                torch.cuda.empty_cache()
                val_results = {}
                val_para = 0
                for vi, val_set in enumerate(self.val_set_list):
                    val_results_cur, val_para_cur = self.val_evaluator(
                        self.network, self.val_losses + self.val_metrics, val_set, step,
                        self.model_name, val_set_name=self.val_set_names[vi])
                    for k, v in val_results_cur.items():
                        val_results[f'{self.val_set_names[vi]}-{k}'] = v
                    # always use the final val set to select model!
                    val_para = val_para_cur

                if val_para > best_para:
                    print(f'New best model {self.cfg["key_metric_name"]}: {val_para:.5f} previous {best_para:.5f}')
                    best_para = val_para
                    self._save_model(step + 1, best_para, self.best_pth_fn)
                self._log_data(val_results, step + 1, 'val')
                del val_results, val_para, val_para_cur, val_results_cur
                
                print((torch.sigmoid(self.network.IORs) * 0.9 + 1))
                IOR_history.append((torch.sigmoid(self.network.IORs) * 0.9 + 1)[0].item())
                print(IOR_history)
               # print(self.network.IORs.grad)

            if (step + 1) % self.cfg['save_interval'] == 0:
                save_fn = None
                self._save_model(step + 1, best_para, save_fn=save_fn)

            pbar.set_postfix(loss=float(loss.detach().cpu().numpy()), lr=lr)
            pbar.update(1)
            del loss, log_info
           
        pbar.close()

    def _load_model(self):
        best_para, start_step = 0, 0
        if os.path.exists(self.pth_fn):
            checkpoint = torch.load(self.pth_fn)
            best_para = checkpoint['best_para']
            start_step = checkpoint['step']
            self.network.load_state_dict(checkpoint['network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f'==> resuming from step {start_step} best para {best_para}')

        return best_para, start_step

    def _save_model(self, step, best_para, save_fn=None):
        save_fn = self.pth_fn if save_fn is None else save_fn
        torch.save({
            'step': step,
            'best_para': best_para,
            'network_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, save_fn)

    def _init_logger(self):
        self.logger = Logger(self.model_dir)

    def _log_data(self, results, step, prefix='train', verbose=False):
        log_results = {}
        for k, v in results.items():
            if isinstance(v, float) or np.isscalar(v):
                log_results[k] = v
            elif type(v) == np.ndarray:
                log_results[k] = np.mean(v)
            else:
                log_results[k] = np.mean(v.detach().cpu().numpy())
        self.logger.log(log_results, prefix, step, verbose)
